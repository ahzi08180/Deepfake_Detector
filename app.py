import streamlit as st
import torch
import torch.nn as nn
from torchvision import models, transforms
from PIL import Image, ImageDraw
from facenet_pytorch import MTCNN
import numpy as np
import cv2
import os

st.set_page_config(page_title="FFT Deepfake Detector", layout="wide")
st.title("ğŸ›¡ï¸ EfficientNet-B0 Deepfake åµæ¸¬ç³»çµ± (FFT + RGB)")

@st.cache_resource
def load_all():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    mtcnn = MTCNN(image_size=224, margin=20, device=device)
    
    # åˆå§‹åŒ– EfficientNet-B0
    model = models.efficientnet_b0(weights=None)

    # === å°é½Š train.pyï¼š4 channel input ===
    old_conv = model.features[0][0]
    model.features[0][0] = nn.Conv2d(
        4,
        old_conv.out_channels,
        kernel_size=old_conv.kernel_size,
        stride=old_conv.stride,
        padding=old_conv.padding,
        bias=False
    )

    # === â­ é—œéµï¼šå°é½Š train.py çš„ classifier ===
    model.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)

    # è¼‰å…¥è¨“ç·´å¥½çš„æ¬Šé‡
    model_path = "rvf10k_efficientnetb0.pth"
    if os.path.exists(model_path):
        model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device).eval()
    
    return mtcnn, model, device

mtcnn, model, device = load_all()

def process_fft(face_pil):
    img_gray = np.array(face_pil.convert('L'))
    f = np.fft.fft2(img_gray)
    fshift = np.fft.fftshift(f)
    magnitude_spectrum = 20 * np.log(np.abs(fshift)+1)
    magnitude_spectrum = cv2.normalize(magnitude_spectrum, None, 0.0, 1.0, cv2.NORM_MINMAX, dtype=cv2.CV_32F)
    magnitude_spectrum = np.clip(magnitude_spectrum, 0.0, 1.0)
    return torch.from_numpy(magnitude_spectrum).float().unsqueeze(0)

uploaded_file = st.file_uploader("ä¸Šå‚³ç…§ç‰‡é€²è¡Œé »åŸŸåˆ†æ...", type=["jpg", "png", "jpeg"])

if uploaded_file:
    img = Image.open(uploaded_file).convert('RGB')
    img_draw = img.copy()
    draw = ImageDraw.Draw(img_draw)

    # ä½¿ç”¨ MTCNN åµæ¸¬äººè‡‰
    boxes, _ = mtcnn.detect(img)

    if boxes is None or len(boxes) == 0:
        st.error("âŒ æœªåµæ¸¬åˆ°äººè‡‰ï¼ŒDeepfake åˆ†æéœ€åŒ…å«æ¸…æ¥šäººè‡‰ã€‚")
        st.info("ğŸ‘‰ å»ºè­°ï¼šæ­£é¢ã€å–®äººã€è‡‰éƒ¨ä½”ç•«é¢ 30% ä»¥ä¸Š")
        st.stop()
    
    # ç•«å‡º bounding box
    for box in boxes:
        draw.rectangle(box.tolist(), outline="red", width=3)

    # å–ç¬¬ä¸€å¼µåµæ¸¬åˆ°çš„äººè‡‰
    x1, y1, x2, y2 = [int(b) for b in boxes[0]]
    face = img.crop((x1, y1, x2, y2))
    face = face.resize((224,224))

    # ç©ºåŸŸ Tensor + Normalize
    normalize = transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
    rgb_tensor = normalize(transforms.ToTensor()(face)).to(device)

    # é »åŸŸ Tensor
    fft_tensor = process_fft(face).to(device)

    # åˆä½µ 4 é€šé“
    input_tensor = torch.cat((rgb_tensor, fft_tensor), dim=0).unsqueeze(0).to(device)

    with torch.no_grad():
        output = model(input_tensor)
        prob = torch.softmax(output, dim=1)[0]

    fake_prob = prob[0].item()
    real_prob = prob[1].item()

    # --- UI é¡¯ç¤º ---
    col1, col2, col3 = st.columns(3)
    with col1:
        st.image(img_draw, caption="åµæ¸¬åˆ°çš„äººè‡‰ & Bounding Box", width='stretch')
    with col2:
        fft_viz = fft_tensor.squeeze().cpu().numpy()
        st.image(fft_viz, caption="FFT é »è­œ (AI å½å½±åµæ¸¬)", clamp=True, width='stretch')
    with col3:
        st.metric("ğŸŸ¥ å½é€ æ©Ÿç‡", f"{fake_prob*100:.2f}%")
        st.metric("ğŸŸ© çœŸå¯¦æ©Ÿç‡", f"{real_prob*100:.2f}%")
        if fake_prob > real_prob:
            st.error("ğŸš¨ åˆ¤å®šç‚º AI ç”Ÿæˆ (Deepfake)")
        else:
            st.success("âœ… åˆ¤å®šç‚ºçœŸå¯¦äººè‡‰")
